{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYmGUmVMkC6GlmX6s1ADUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apples-17/Decibel_duel_PS/blob/main/Deciduel_final_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0rrFnBxUTVz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Listing everything under MyDrive:\\n\")\n",
        "print(os.listdir('/content/drive/MyDrive'))"
      ],
      "metadata": {
        "id": "dbN8rkriUWbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/MyDrive/Decibel Duel'\n",
        "train_dir = f'{base_dir}/train/train'\n",
        "test_dir = f'{base_dir}/test/test'\n",
        "out_submission=f'{base_dir}/submission.csv'"
      ],
      "metadata": {
        "id": "F_T0w3vUUYRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Test folder exists:\", os.path.exists(test_dir))\n",
        "print(\"Number of test files:\", len(os.listdir(test_dir)))\n",
        "\n",
        "# Show a few filenames\n",
        "print(\"Sample files:\", os.listdir(test_dir)[:5])"
      ],
      "metadata": {
        "id": "ZyC6j7AeUbNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_r-ifs7iUdx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path=f'{train_dir}/dog_bark/344-3-1-0.wav'"
      ],
      "metadata": {
        "id": "4GQTjjEjUf7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "librosa_audio_data,librosa_audio_sample_rate=librosa.load(audio_file_path)\n",
        "librosa.display.waveshow(librosa_audio_data, sr=librosa_audio_sample_rate)\n",
        "ipd.Audio(audio_file_path)"
      ],
      "metadata": {
        "id": "pi5MstYzUhwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "librosa_audio_sample_rate"
      ],
      "metadata": {
        "id": "p4H3EctHUkDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "librosa_audio_data"
      ],
      "metadata": {
        "id": "M17mjNLdUl8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import wavfile as wav\n",
        "wave_sample_rate, wave_audio=wav.read(audio_file_path)\n",
        "wave_audio"
      ],
      "metadata": {
        "id": "9WcvZJoLUno_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(wave_audio)"
      ],
      "metadata": {
        "id": "sqyN0VQlUpnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MFCC-Mel Frequency Cepstral Coefficients\n",
        "mfccs=librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_audio_sample_rate)\n",
        "mfccs.shape"
      ],
      "metadata": {
        "id": "5GvuIminUrbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfccs"
      ],
      "metadata": {
        "id": "acba3q5gUtZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dataset_path=f'{train_dir}/'"
      ],
      "metadata": {
        "id": "CNiLZLsTUw8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def features_extractor(file_name):\n",
        "  audio, sample_rate=librosa.load(file_name, res_type='kaiser_fast')\n",
        "  mfccs_features=librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "  mfccs_scaled_features=np.mean(mfccs_features.T, axis=0)\n",
        "  return mfccs_scaled_features"
      ],
      "metadata": {
        "id": "qoo92vlgUzCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install resampy"
      ],
      "metadata": {
        "id": "bLCleiXDU0LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import resampy"
      ],
      "metadata": {
        "id": "XIE6bV2IU19s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "extracted_features = []\n",
        "for class_label in os.listdir(train_dir):\n",
        "    class_path = os.path.join(train_dir, class_label)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    for file_name in tqdm(os.listdir(class_path), desc=f\"Processing {class_label}\"):\n",
        "        file_path = os.path.join(class_path, file_name)\n",
        "\n",
        "        try:\n",
        "            data = features_extractor(file_path)\n",
        "            extracted_features.append([data, class_label])\n",
        "        except Exception as e:\n",
        "            print(f\"Error with file {file_name}: {e}\")"
      ],
      "metadata": {
        "id": "ZWzXcefvU3zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_features_df=pd.DataFrame(extracted_features, columns=['features', 'class'])\n",
        "extracted_features_df.head()"
      ],
      "metadata": {
        "id": "d6WsIHKMU6K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array(extracted_features_df['features'].tolist())\n",
        "y=np.array(extracted_features_df['class'].tolist())"
      ],
      "metadata": {
        "id": "t-YsyuTDVzem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "mH-7fG0zV6fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder=LabelEncoder()\n",
        "y=to_categorical(labelencoder.fit_transform(y))"
      ],
      "metadata": {
        "id": "DBeJ9ZONV-Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "RPoomcDCWCFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "_8LjZ0xLWCsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "xCmFmP7dWEsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "7x5hTQQ1WHMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "I6Bd5PIdWJWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "16XpxqtWWLIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "TJ1kfnctWM4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_enhanced_melspec(file_path, sr=22050, duration=5, n_mels=128, augment=False):\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
        "\n",
        "        if augment and np.random.random() > 0.5:\n",
        "            if np.random.random() > 0.5:\n",
        "                rate = np.random.uniform(0.9, 1.1)\n",
        "                y = librosa.effects.time_stretch(y, rate=rate)\n",
        "\n",
        "            if np.random.random() > 0.5:\n",
        "                n_steps = np.random.randint(-2, 3)\n",
        "                y = librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)\n",
        "\n",
        "        target_length = sr * duration\n",
        "        if len(y) < target_length:\n",
        "            y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
        "        else:\n",
        "            y = y[:target_length]\n",
        "\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=y, sr=sr, n_mels=n_mels, fmax=8000, n_fft=2048, hop_length=512\n",
        "        )\n",
        "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-6)\n",
        "\n",
        "        return mel_spec_db\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "X_train_all = []\n",
        "y_train_all = []\n",
        "\n",
        "for class_label in os.listdir(train_dir):\n",
        "    class_path = os.path.join(train_dir, class_label)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    files = [f for f in os.listdir(class_path) if f.endswith(('.wav', '.mp3'))]\n",
        "\n",
        "    for file_name in tqdm(files, desc=f\"Processing {class_label}\"):\n",
        "        file_path = os.path.join(class_path, file_name)\n",
        "\n",
        "        mel_spec = extract_enhanced_melspec(file_path, augment=False)\n",
        "        if mel_spec is not None:\n",
        "            X_train_all.append(mel_spec)\n",
        "            y_train_all.append(class_label)\n",
        "\n",
        "        mel_spec_aug = extract_enhanced_melspec(file_path, augment=True)\n",
        "        if mel_spec_aug is not None:\n",
        "            X_train_all.append(mel_spec_aug)\n",
        "            y_train_all.append(class_label)\n",
        "\n",
        "X_train_all = np.array(X_train_all)\n",
        "X_train_all = X_train_all[..., np.newaxis]\n",
        "\n",
        "print(f\"\\nTotal samples with augmentation: {len(X_train_all)}\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = to_categorical(le.fit_transform(y_train_all))"
      ],
      "metadata": {
        "id": "9MF6WHRIWPWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL\n",
        "def build_enhanced_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "\n",
        "        # 1\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        # 2\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.2),\n",
        "\n",
        "        # 3\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # 4\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # 5\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "\n",
        "        layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "buvc9aixWy0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 5\n",
        "models_list = []\n",
        "fold_scores = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "y_labels = np.argmax(y_encoded, axis=1)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_all, y_labels), 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"FOLD {fold}/{n_splits}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    X_tr, X_val = X_train_all[train_idx], X_train_all[val_idx]\n",
        "    y_tr, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    model = build_enhanced_model(X_train_all.shape[1:], y_encoded.shape[1])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    early_stop = callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=20,\n",
        "        restore_best_weights=True,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=7,\n",
        "        min_lr=1e-7,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_tr, y_tr,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f\"Fold {fold} Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    models_list.append(model)\n",
        "    fold_scores.append(val_acc)\n",
        "\n",
        "    model.save(f'{base_dir}/model_fold_{fold}.h5')\n",
        "\n",
        "print(f\"Individual fold accuracies: {[f'{s:.4f}' for s in fold_scores]}\")\n",
        "print(f\"Mean CV Accuracy: {np.mean(fold_scores):.4f} (Â±{np.std(fold_scores):.4f})\")"
      ],
      "metadata": {
        "id": "m085dWMDW3oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS IS JUST A CODE FOR MODEL LOADING. I RAN OUT OF GPU AT THAT TIME, SO I USED IT THE SAVED MODEL. HONESTLY, IDK IF IT'S RIGHT. BUT THAT'S ALL I THINK.\n",
        "\n",
        "# from tensorflow.keras.models import load_model\n",
        "# import os\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# base_dir = '/content/drive/MyDrive/Decibel Duel'\n",
        "# n_splits = 5\n",
        "\n",
        "# model_paths = [os.path.join(base_dir, f'model_fold_{i}.h5') for i in range(1, n_splits+1)]\n",
        "# missing = [p for p in model_paths if not os.path.exists(p)]\n",
        "# if missing:\n",
        "#     raise FileNotFoundError(f\"Missing model files: {missing}\")\n",
        "\n",
        "# class_names = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# le.fit(class_names)\n",
        "\n",
        "# models_list = []\n",
        "# for p in model_paths:\n",
        "#     print(f\"Loading {p} ...\")\n",
        "#     model = load_model(p)\n",
        "#     models_list.append(model)\n",
        "\n",
        "# print(f\"Loaded {len(models_list)} models. Ready for ensemble + TTA.\")\n"
      ],
      "metadata": {
        "id": "_tbp4Nb_W4jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_files = sorted([f for f in os.listdir(test_dir) if f.endswith(('.wav', '.mp3'))])\n",
        "predictions = []\n",
        "\n",
        "print(f\"\\nProcessing {len(test_files)} test files...\")\n",
        "\n",
        "for file_name in tqdm(test_files):\n",
        "    file_path = os.path.join(test_dir, file_name)\n",
        "\n",
        "    all_preds = []\n",
        "\n",
        "    for model in models_list:\n",
        "        mel_spec = extract_enhanced_melspec(file_path, augment=False)\n",
        "        if mel_spec is not None:\n",
        "            mel_input = mel_spec[np.newaxis, ..., np.newaxis]\n",
        "            pred = model.predict(mel_input, verbose=0)\n",
        "            all_preds.append(pred[0])\n",
        "\n",
        "        for _ in range(2):\n",
        "            mel_spec_aug = extract_enhanced_melspec(file_path, augment=True)\n",
        "            if mel_spec_aug is not None:\n",
        "                mel_input = mel_spec_aug[np.newaxis, ..., np.newaxis]\n",
        "                pred = model.predict(mel_input, verbose=0)\n",
        "                all_preds.append(pred[0])\n",
        "\n",
        "    if len(all_preds) > 0:\n",
        "        avg_pred = np.mean(all_preds, axis=0)\n",
        "        pred_class_idx = np.argmax(avg_pred)\n",
        "        pred_class = le.classes_[pred_class_idx]\n",
        "\n",
        "        predictions.append({\n",
        "            'ID': file_name,\n",
        "            'Class': pred_class\n",
        "        })\n",
        "\n",
        "# Save submission\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "output_file = f'{base_dir}/submission.csv'\n",
        "submission_df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "0SzBtYbAW9HJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}